{
    "docs": [
        {
            "location": "/",
            "text": "Test Tube: Easily log and tune Deep Learning experiments\n\n\nTest Tube allows you to easily log metadata and track your machine learning experiments.\n\n\nUse Test Tube if you need to:\n\n\n\n\nTrack many \nExperiments\n across models.\n\n\nVisualize\n and compare different experiments without uploading anywhere.\n\n\nOptimize your hyperparameters\n using grid_search or random_search.\n\n\nAutomatically track ALL parameters for a particular training run.\n\n\n\n\nTest Tube is compatible with: Python 2 and 3\n\n\nGetting started\n\n\n\n\nCreate an \nExperiment\n\n\nfrom test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.add_meta_tags({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.add_metric_row('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com\n\n\n\n\n\n\nOptimize your \nhyperparameters\n\n\nfrom test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)\n\n\n\n\n\n\nVisualize\n\n\nimport pandas as pd\nimport matplotlib\n\n# each experiment is saved to a metrics.csv file which can be imported anywhere\n# images save to exp/version/images\ndf = pd.read_csv('../some/dir/test_tube_data/dense_model/version_0/metrics.csv')\ndf.tng_err.plot()",
            "title": "Test Tube: Easily log and tune Deep Learning experiments"
        },
        {
            "location": "/#test-tube-easily-log-and-tune-deep-learning-experiments",
            "text": "Test Tube allows you to easily log metadata and track your machine learning experiments.  Use Test Tube if you need to:   Track many  Experiments  across models.  Visualize  and compare different experiments without uploading anywhere.  Optimize your hyperparameters  using grid_search or random_search.  Automatically track ALL parameters for a particular training run.   Test Tube is compatible with: Python 2 and 3",
            "title": "Test Tube: Easily log and tune Deep Learning experiments"
        },
        {
            "location": "/#getting-started",
            "text": "",
            "title": "Getting started"
        },
        {
            "location": "/#create-an-experiment",
            "text": "from test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.add_meta_tags({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.add_metric_row('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com",
            "title": "Create an Experiment"
        },
        {
            "location": "/#optimize-your-hyperparameters",
            "text": "from test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)",
            "title": "Optimize your hyperparameters"
        },
        {
            "location": "/#visualize",
            "text": "import pandas as pd\nimport matplotlib\n\n# each experiment is saved to a metrics.csv file which can be imported anywhere\n# images save to exp/version/images\ndf = pd.read_csv('../some/dir/test_tube_data/dense_model/version_0/metrics.csv')\ndf.tng_err.plot()",
            "title": "Visualize"
        },
        {
            "location": "/experiment_tracking/experiment/",
            "text": "Experiment class API\n\n\nAn Experiment holds metadata and the results of the training run, you can instantiate an \nExperiment\n via:\n\n\nfrom test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.add_meta_tags({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.add_metric_row('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com\n\n\n\n\n\n\ninit options\n\n\nversion\n\n\nThe same Experiment can have multiple versions. Test tube generates these automatically each time you run your model. To set your own version use:\n\n\nexp = Experiment(name='dense_model',version=1)\n\n\n\n\ndebug\n\n\nIf you're debugging and don't want to create a log file turn debug to True\n\n\nexp = Experiment(name='dense_model',debug=True)\n\n\n\n\nautosave\n\n\nIf you only want to save at the end of training, turn autosave off:\n\n\nexp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()\n\n\n\n\ncreate_git_tag\n\n\nEver wanted a flashback to your code when you ran an experiment?\nSnapshot your code for this experiment using git tags:\n\n\nexp = Experiment(name='dense_model', create_git_tag=True)\n\n\n\n\n\n\nMethods\n\n\nadd_meta_tag\n\n\nexp.add_meta_tag(key, val)\n\n\n\n\nAdds an arbitrary tag and value to your experiment\n\n\nExample\n\n\nexp.add_meta_tag('dataset_name', 'imagenet_1')\nexp.add_meta_tag('learning_rate', 0.002)\n\n\n\n\nadd_meta_tags\n\n\nexp.add_meta_tags({k: v})\n\n\n\n\nAdds an arbitrary dictionary of tags to the experiment\n\n\nExample\n\n\nexp.add_meta_tags({'dataset_name': 'imagenet_1', 'learning_rate': 0.0002})\n\n\n\n\nadd_metric_row\n\n\nexp.add_metric_row({k:v})\n\n\n\n\nAdds a row of data to the experiments\n\n\nExample\n\n\nexp.add_metric_row({'val_loss': 0.22, 'epoch_nb': 1, 'batch_nb': 12})\n\n# you can also add other rows that have separate information\nexp.add_metric_row({'tng_loss': 0.01})\n\n# or even a numpy array image\nimage = np.imread('image.png')\nexp.add_metric_row({'fake_png': image})\n\n\n\n\nSaving images Example\n\n\n# name must have either jpg, png or jpeg in it\nimg = np.imread('a.jpg')\nexp.add_metric_row('test_jpg': img, 'val_err': 0.2)\n\n# saves image to ../exp/version/media/test_0.jpg\n# csv has file path to that image in that cell\n\n\n\n\nTo save an image, add \njpg\n, \npng\n or \njpeg\n to the key corresponding with the image array. The image must be formatted the same as skimage's \nimsave\n function\n\n\nadd_argparse_meta\n\n\nexp.add_argparse_meta(hparams)\n\n\n\n\nTransfers hyperparam information from Argparser or HyperOptArgumentParser\n\n\nExample\n\n\nfrom test_tube import HyperOptArgumentParser\n\n# parse args\nparser = HyperOptArgumentParser()\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nhparams = parser.parse_args()\n\n# learning_rate is now a meta tag for your experiment\nexp.add_argparse_meta(hparams)\n\n\n\n\nsave\n\n\nexp.save()\n\n\n\n\nSaves the exp to disk (including images)\n\n\nExample\n\n\nexp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()",
            "title": "Experiment class API"
        },
        {
            "location": "/experiment_tracking/experiment/#experiment-class-api",
            "text": "An Experiment holds metadata and the results of the training run, you can instantiate an  Experiment  via:  from test_tube import Experiment\n\nexp = Experiment(name='dense_model',\n                 debug=False,\n                 save_dir='/Desktop/test_tube')\n\nexp.add_meta_tags({'learning_rate': 0.002, 'nb_layers': 2})\n\nfor step in training_steps:\n    tng_err = model.eval(tng_x, tng_y)\n\n    exp.add_metric_row('tng_err': tng_err)\n\n# training complete!\n# all your logs and data are ready to be visualized at testtube.williamfalcon.com",
            "title": "Experiment class API"
        },
        {
            "location": "/experiment_tracking/experiment/#init-options",
            "text": "",
            "title": "init options"
        },
        {
            "location": "/experiment_tracking/experiment/#version",
            "text": "The same Experiment can have multiple versions. Test tube generates these automatically each time you run your model. To set your own version use:  exp = Experiment(name='dense_model',version=1)",
            "title": "version"
        },
        {
            "location": "/experiment_tracking/experiment/#debug",
            "text": "If you're debugging and don't want to create a log file turn debug to True  exp = Experiment(name='dense_model',debug=True)",
            "title": "debug"
        },
        {
            "location": "/experiment_tracking/experiment/#autosave",
            "text": "If you only want to save at the end of training, turn autosave off:  exp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()",
            "title": "autosave"
        },
        {
            "location": "/experiment_tracking/experiment/#create_git_tag",
            "text": "Ever wanted a flashback to your code when you ran an experiment?\nSnapshot your code for this experiment using git tags:  exp = Experiment(name='dense_model', create_git_tag=True)",
            "title": "create_git_tag"
        },
        {
            "location": "/experiment_tracking/experiment/#methods",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/experiment_tracking/experiment/#add_meta_tag",
            "text": "exp.add_meta_tag(key, val)  Adds an arbitrary tag and value to your experiment  Example  exp.add_meta_tag('dataset_name', 'imagenet_1')\nexp.add_meta_tag('learning_rate', 0.002)",
            "title": "add_meta_tag"
        },
        {
            "location": "/experiment_tracking/experiment/#add_meta_tags",
            "text": "exp.add_meta_tags({k: v})  Adds an arbitrary dictionary of tags to the experiment  Example  exp.add_meta_tags({'dataset_name': 'imagenet_1', 'learning_rate': 0.0002})",
            "title": "add_meta_tags"
        },
        {
            "location": "/experiment_tracking/experiment/#add_metric_row",
            "text": "exp.add_metric_row({k:v})  Adds a row of data to the experiments  Example  exp.add_metric_row({'val_loss': 0.22, 'epoch_nb': 1, 'batch_nb': 12})\n\n# you can also add other rows that have separate information\nexp.add_metric_row({'tng_loss': 0.01})\n\n# or even a numpy array image\nimage = np.imread('image.png')\nexp.add_metric_row({'fake_png': image})  Saving images Example  # name must have either jpg, png or jpeg in it\nimg = np.imread('a.jpg')\nexp.add_metric_row('test_jpg': img, 'val_err': 0.2)\n\n# saves image to ../exp/version/media/test_0.jpg\n# csv has file path to that image in that cell  To save an image, add  jpg ,  png  or  jpeg  to the key corresponding with the image array. The image must be formatted the same as skimage's  imsave  function",
            "title": "add_metric_row"
        },
        {
            "location": "/experiment_tracking/experiment/#add_argparse_meta",
            "text": "exp.add_argparse_meta(hparams)  Transfers hyperparam information from Argparser or HyperOptArgumentParser  Example  from test_tube import HyperOptArgumentParser\n\n# parse args\nparser = HyperOptArgumentParser()\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nhparams = parser.parse_args()\n\n# learning_rate is now a meta tag for your experiment\nexp.add_argparse_meta(hparams)",
            "title": "add_argparse_meta"
        },
        {
            "location": "/experiment_tracking/experiment/#save",
            "text": "exp.save()  Saves the exp to disk (including images)  Example  exp = Experiment(name='dense_model', autosave=False)\n\n# run long training...\n\n# first time any logs are saved\nexp.save()",
            "title": "save"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/",
            "text": "HyperOptArgumentParser class API\n\n\nThe HyperOptArgumentParser is a subclass of python's \nargparse\n, with added finctionality to change parameters on the fly as determined by a sampling strategy.\n\n\nYou can instantiate an \nHyperOptArgumentParser\n via:\n\n\nfrom test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)\n\n\n\n\n\n\ninit options\n\n\nstrategy\n\n\nUse either \nrandom search\n or \ngrid search\n for tuning:\n\n\nparser = HyperOptArgumentParser(strategy='grid_search')\n\n\n\n\n\n\nMethods\n\n\nAll the functionality from argparse works but we've added the following functionality:\n\n\nadd_opt_argument_list\n\n\nparser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])\n\n\n\n\nEnables searching over a list of values for this parameter. The tunable values ONLY replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it.\n\n\nExample\n\n\nparser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])\nhparams = parser.parse_args()\n# hparams.nb_layers = 2\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [2, 4, 8]\n    # but hparams.nb_layers is still 2\n\n\n\n\n\nadd_opt_argument_range\n\n\nparser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=8)\n\n\n\n\nEnables searching over a range of values chosen linearly using the nb_samples given. The tunable values ONLY replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it.\n\n\nExample\n\n\nparser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=8)\nhparams = parser.parse_args()\n# hparams.neurons = 50\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [100, 200, 300, 400, 500, 600 700, 800]\n    # but hparams.neurons is still 50\n\n\n\n\nadd_json_config_argument\n\n\nparser.add_json_config_argument('--config', default='example.json')\n\n\n\n\nReplaces default values in the parser with those read from the json file\n\n\nExample\n\n\nexample.json\n\n\n{\n    \"learning_rate\": 200\n}\n\n\n\n\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nparser.add_json_config_argument('--config', default='example.json')\nhparams = parser.parse_args()\n\n# hparams.learning_rate = 200\n\n\n\n\ntrials\n\n\ntrial_generator = hparams.trials(2)\n\n\n\n\nComputes the trials needed for these experiments and serves them via a generator\n\n\nExample\n\n\nhparams = parser.parse_args()\nfor trial_hparams in hparams.trials(2):\n    # trial_hparams now has values sampled from the training routine\n\n\n\n\noptimize_parallel\n\n\nDEPRECATED... see optimize_parallel_gpu / _cpu\n\n\nhparams = parser.parse_args()\nhparams.optimize_parallel(function_to_optimize, nb_trials=20, nb_parallel=2)\n\n\n\n\nParallelize the trials across nb_parallel processes.\nArguments passed into the \nfunction_to_optimize\n are the \ntrial_params\n and index of process it's in.\n\n\nExample\n\n\n# parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef opt_function(trial_params, process_index):\n    GPUs = ['0', '1']\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPUs[process_index]\n    train_main(trial_params)\n\nhparams = parser.parse_args()\nhparams.optimize_parallel(opt_function, nb_trials=20, nb_parallel=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel\n\n\n\n\noptimize_parallel_gpu_cuda\n\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_gpu_cuda(function_to_optimize, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)\n\n\n\n\nParallelize the trials across nb_workers processes. Auto assign the correct gpus.\nArgument passed into the \nfunction_to_optimize\n is the \ntrial_params\n argument.\n\n\nExample\n\n\n# parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_gpu_cuda(train_main, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel\n\n\n\n\noptimize_parallel_cpu\n\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_cpu(function_to_optimize, nb_trials=20, nb_workers=2)\n\n\n\n\nParallelize the trials across nb_workers cpus.\nArgument passed into the \nfunction_to_optimize\n is the \ntrial_params\n argument.\n\n\nExample\n\n\n# parallelize tuning on 2 cpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_cpu(train_main, nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "HyperOptArgumentParser class API"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#hyperoptargumentparser-class-api",
            "text": "The HyperOptArgumentParser is a subclass of python's  argparse , with added finctionality to change parameters on the fly as determined by a sampling strategy.  You can instantiate an  HyperOptArgumentParser  via:  from test_tube import HyperOptArgumentParser\n\n# subclass of argparse\nparser = HyperOptArgumentParser(strategy='random_search')\nparser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\n\n# let's enable optimizing over the number of layers in the network\nparser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])\n\n# and tune the number of units in each layer\nparser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=10)\n\n# compile (because it's argparse underneath)\nhparams = parser.parse_args()\n\n# run 20 trials of random search over the hyperparams\nfor hparam_trial in hparams.trials(20):\n    train_network(hparam_trial)",
            "title": "HyperOptArgumentParser class API"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#init-options",
            "text": "",
            "title": "init options"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#strategy",
            "text": "Use either  random search  or  grid search  for tuning:  parser = HyperOptArgumentParser(strategy='grid_search')",
            "title": "strategy"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#methods",
            "text": "All the functionality from argparse works but we've added the following functionality:",
            "title": "Methods"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#add_opt_argument_list",
            "text": "parser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])  Enables searching over a list of values for this parameter. The tunable values ONLY replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it.  Example  parser.add_opt_argument_list('--nb_layers', default=2, type=int, tunnable=True, options=[2, 4, 8])\nhparams = parser.parse_args()\n# hparams.nb_layers = 2\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [2, 4, 8]\n    # but hparams.nb_layers is still 2",
            "title": "add_opt_argument_list"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#add_opt_argument_range",
            "text": "parser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=8)  Enables searching over a range of values chosen linearly using the nb_samples given. The tunable values ONLY replace the argparse values when running a hyperparameter optimization search. This is on purpose so your code doesn't have to change when you want to tune it.  Example  parser.add_opt_argument_range('--neurons', default=50, type=int, tunnable=True, start=100, end=800, nb_samples=8)\nhparams = parser.parse_args()\n# hparams.neurons = 50\n\nfor trial in hparams.trials(2):\n    # trial.nb_layers is now a value in [100, 200, 300, 400, 500, 600 700, 800]\n    # but hparams.neurons is still 50",
            "title": "add_opt_argument_range"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#add_json_config_argument",
            "text": "parser.add_json_config_argument('--config', default='example.json')  Replaces default values in the parser with those read from the json file  Example  example.json  {\n    \"learning_rate\": 200\n}  parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')\nparser.add_json_config_argument('--config', default='example.json')\nhparams = parser.parse_args()\n\n# hparams.learning_rate = 200",
            "title": "add_json_config_argument"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#trials",
            "text": "trial_generator = hparams.trials(2)  Computes the trials needed for these experiments and serves them via a generator  Example  hparams = parser.parse_args()\nfor trial_hparams in hparams.trials(2):\n    # trial_hparams now has values sampled from the training routine",
            "title": "trials"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel",
            "text": "DEPRECATED... see optimize_parallel_gpu / _cpu  hparams = parser.parse_args()\nhparams.optimize_parallel(function_to_optimize, nb_trials=20, nb_parallel=2)  Parallelize the trials across nb_parallel processes.\nArguments passed into the  function_to_optimize  are the  trial_params  and index of process it's in.  Example  # parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef opt_function(trial_params, process_index):\n    GPUs = ['0', '1']\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPUs[process_index]\n    train_main(trial_params)\n\nhparams = parser.parse_args()\nhparams.optimize_parallel(opt_function, nb_trials=20, nb_parallel=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "optimize_parallel"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel_gpu_cuda",
            "text": "hparams = parser.parse_args()\nhparams.optimize_parallel_gpu_cuda(function_to_optimize, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)  Parallelize the trials across nb_workers processes. Auto assign the correct gpus.\nArgument passed into the  function_to_optimize  is the  trial_params  argument.  Example  # parallelize tuning on 2 gpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_gpu_cuda(train_main, gpu_ids=['1', '0, 2'], nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "optimize_parallel_gpu_cuda"
        },
        {
            "location": "/hyperparameter_optimization/HyperOptArgumentParser/#optimize_parallel_cpu",
            "text": "hparams = parser.parse_args()\nhparams.optimize_parallel_cpu(function_to_optimize, nb_trials=20, nb_workers=2)  Parallelize the trials across nb_workers cpus.\nArgument passed into the  function_to_optimize  is the  trial_params  argument.  Example  # parallelize tuning on 2 cpus\n# this will place each trial in n into a given gpu\ndef train_main(trial_params):\n    # train your model, etc here...\n\nhparams = parser.parse_args()\nhparams.optimize_parallel_cpu(train_main, nb_trials=20, nb_workers=2)\n\n# at the end of the optimize_parallel function, all 20 trials will be completed\n# in this case by running 10 sets of 2 trials in parallel",
            "title": "optimize_parallel_cpu"
        }
    ]
}